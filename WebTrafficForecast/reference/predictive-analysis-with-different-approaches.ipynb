{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "906ebb36-965e-4f40-8631-6e2b0542b513",
    "_uuid": "a68b5b7e17491b447693cde391f5b4551bd7bdb1"
   },
   "source": [
    "# <h1><center> Predictive Analysis - Web Traffic Time Series Forecasting | Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "183205d7-ec25-4181-8257-f844ff800371",
    "_uuid": "90047c203a2d26eb05677150191a4306b670f26e"
   },
   "source": [
    "The goal of this notebook is not to do the best model for each Time series. It is just a comparison of few models when you have one Time Series. The presentation present a different approaches to forecast a Time Series. \n",
    "\n",
    "The plan of the notebook is:\n",
    "\n",
    "    I. Importation & Data Cleaning\n",
    "    II. Aggregation & Visualisation\n",
    "    III. Machine Learning Approach\n",
    "    IV Basic Model Approach\n",
    "    V. ARIMA approach (Autoregressive Integrated Moving Average)\n",
    "    VI. (FB) Prophet Approach \n",
    "    VII. Keras Starter\n",
    "    VIII. Comparaison & Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b75bda5-f598-4398-9257-65919d191d17",
    "_uuid": "5cdf05e798fefe9f1f48ca2c8b22a40e643491b2"
   },
   "source": [
    "#  <h1><center> I. Importation & Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8a25293d-0cb2-4c89-8a7d-e0e29246c327",
    "_uuid": "96e4586ea29ce54eb8279580437f6605a6385ead"
   },
   "source": [
    "In this first part we will choose the Time Series to work in the others parts. The idea is to find a Time Serie who could be interesting to work with. So in the data we can find 145K Time Series. We will Find a good Time Series to introduce four approaches! So the first step is to import few libraries and the data. The four approaches are Basic Approach / ML Approach / GAM Approach / ARIMA Approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "f18d917f-908c-4dd0-8949-20e758c34d4f",
    "_execution_state": "idle",
    "_uuid": "40d0359af5152164dc52862d1132e20aec62de21"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import scipy\n",
    "from datetime import timedelta\n",
    "\n",
    "# Forceasting with decompasable model\n",
    "from pylab import rcParams\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# For marchine Learning Approach\n",
    "from statsmodels.tsa.tsatools import lagmat\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "885a9dc5-e6e8-49a1-90cc-0c820610a3e2",
    "_execution_state": "idle",
    "_uuid": "07a9956525cfa8d1c515f1c7ddd048e0052f8732",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv(\"../input/train_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "d9a9ea52-fec8-44d2-a107-24f67cdfe15a",
    "_execution_state": "idle",
    "_uuid": "9c01781c2d8c0bdec2d2b09b3e60440c6d040fcf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_flattened = pd.melt(train[list(train.columns[-50:])+['Page']], id_vars='Page', var_name='date', value_name='Visits')\n",
    "train_flattened['date'] = train_flattened['date'].astype('datetime64[ns]')\n",
    "train_flattened['weekend'] = ((train_flattened.date.dt.dayofweek) // 5 == 1).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "136e7f45-b413-46c8-bccc-7fc9787545e4",
    "_execution_state": "idle",
    "_uuid": "45912033d97679806073405a44aae50f3854cee1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Median by page\n",
    "df_median = pd.DataFrame(train_flattened.groupby(['Page'])['Visits'].median())\n",
    "df_median.columns = ['median']\n",
    "\n",
    "# Average by page\n",
    "df_mean = pd.DataFrame(train_flattened.groupby(['Page'])['Visits'].mean())\n",
    "df_mean.columns = ['mean']\n",
    "\n",
    "# Merging data\n",
    "train_flattened = train_flattened.set_index('Page').join(df_mean).join(df_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "5c62a101-22e1-4985-9f76-355d6db0ffbd",
    "_execution_state": "idle",
    "_uuid": "29d3acb1fa0337f5038b39c5c43485689b0886bf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_flattened.reset_index(drop=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "0cc69b1a-ff75-4f6b-b397-5a509f7b816e",
    "_execution_state": "idle",
    "_uuid": "911202f5907ebd66b47851639fb279f5df0d31f7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_flattened['weekday'] = train_flattened['date'].apply(lambda x: x.weekday())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "9889cd36-9708-4f12-8013-129a4e9e2bf1",
    "_execution_state": "idle",
    "_uuid": "ab9703a57d5ea7dd75c88154eada1deb74d5b1dd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature engineering with the date\n",
    "train_flattened['year']=train_flattened.date.dt.year \n",
    "train_flattened['month']=train_flattened.date.dt.month \n",
    "train_flattened['day']=train_flattened.date.dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "937b2986-f29e-4812-8b87-e01eac9a6e83",
    "_execution_state": "idle",
    "_uuid": "8625de34beed2d4bb1d8b3c7d408f1f318ea7d71"
   },
   "outputs": [],
   "source": [
    "train_flattened.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9cd820bd-0af3-4cd9-858a-f643a1bbd72a",
    "_uuid": "9ab974fb95108000188df923468301d89c740476"
   },
   "source": [
    "This part allowed us to prepare our data. We had created new features that we use in the next steps. Days, Months, Years are interesting to forecast with a Machine Learning Approach or to do an analysis. \n",
    "If you have another idea to improve this first part: Fork this notebook and improve it or share your idea in the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b2f8e204-2835-480a-8698-7f41cf3700cf",
    "_uuid": "5116f1908129dab5dc6b6ce0d480ba0ca0f773e9"
   },
   "source": [
    "# <h1><center>II. Aggregation & Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "0a507d6f-d909-40cf-ad5c-ebf26bc490bf",
    "_execution_state": "idle",
    "_uuid": "356ba68d91bd131cffe74d5a2a54f98da4c65145",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 8))\n",
    "mean_group = train_flattened[['Page','date','Visits']].groupby(['date'])['Visits'].mean()\n",
    "plt.plot(mean_group)\n",
    "plt.title('Time Series - Average')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "f3fb5d11-0fbe-4241-adcc-2f7f4a6a377a",
    "_execution_state": "idle",
    "_uuid": "d75d8e8dec30c50af77c4448b4b2eed1f1628f41",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 8))\n",
    "median_group = train_flattened[['Page','date','Visits']].groupby(['date'])['Visits'].median()\n",
    "plt.plot(median_group, color = 'r')\n",
    "plt.title('Time Series - median')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "5b301da9-c8cb-4cef-8f93-ef24b4a9021f",
    "_execution_state": "idle",
    "_uuid": "72a7a488e1a81984940d6fca47b177060e255f62"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 8))\n",
    "std_group = train_flattened[['Page','date','Visits']].groupby(['date'])['Visits'].std()\n",
    "plt.plot(std_group, color = 'g')\n",
    "plt.title('Time Series - std')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "15ae2ff2-f3b1-4ecb-a821-a1b0e9c4f0cd",
    "_execution_state": "idle",
    "_uuid": "06523b6f02f7c5383aa2cfaf700db8d947bdae33",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For the next graphics\n",
    "train_flattened['month_num'] = train_flattened['month']\n",
    "train_flattened['month'].replace('11','11 - November',inplace=True)\n",
    "train_flattened['month'].replace('12','12 - December',inplace=True)\n",
    "\n",
    "train_flattened['weekday_num'] = train_flattened['weekday']\n",
    "train_flattened['weekday'].replace(0,'01 - Monday',inplace=True)\n",
    "train_flattened['weekday'].replace(1,'02 - Tuesday',inplace=True)\n",
    "train_flattened['weekday'].replace(2,'03 - Wednesday',inplace=True)\n",
    "train_flattened['weekday'].replace(3,'04 - Thursday',inplace=True)\n",
    "train_flattened['weekday'].replace(4,'05 - Friday',inplace=True)\n",
    "train_flattened['weekday'].replace(5,'06 - Saturday',inplace=True)\n",
    "train_flattened['weekday'].replace(6,'07 - Sunday',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "738022d0-29d9-433a-9b89-a9a6b9f28590",
    "_execution_state": "idle",
    "_uuid": "80d1d8fa3d5b6f66bdb0d8c8600b5645f369b74e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_group = train_flattened.groupby([\"month\", \"weekday\"])['Visits'].mean().reset_index()\n",
    "train_group = train_group.pivot('weekday','month','Visits')\n",
    "train_group.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "ff193837-2a99-457f-aee0-11c33e5615f9",
    "_execution_state": "idle",
    "_uuid": "3d78c5e0d0e0497bb6fa7b9fc4ce94cbbee404d1"
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=3.5) \n",
    "\n",
    "# Draw a heatmap with the numeric values in each cell\n",
    "f, ax = plt.subplots(figsize=(50, 30))\n",
    "sns.heatmap(train_group, annot=False, ax=ax, fmt=\"d\", linewidths=2)\n",
    "plt.title('Web Traffic Months cross Weekdays')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7faf35c3-0e81-4e76-8cbb-7916e2983fb2",
    "_uuid": "3f86f8a48a64bdffcd5f7266df4bb85baaaf1c2c"
   },
   "source": [
    "This heatmap show us in average the web traffic by weekdays cross the months. In our data we can see there are less activity in Friday and Saturday for December and November. And the biggest traffic is on the period Monday - Wednesday. It is possible to do Statistics Test to check if our intuition is ok. But You have a lot of works ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "6cb5a16a-b36d-4aad-bb3f-f79dc17cb210",
    "_execution_state": "idle",
    "_uuid": "5545a7d5c8ace5362f3e220dcc6e96f4e96fc988",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_day = train_flattened.groupby([\"month\", \"day\"])['Visits'].mean().reset_index()\n",
    "train_day = train_day.pivot('day','month','Visits')\n",
    "train_day.sort_index(inplace=True)\n",
    "train_day.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "5b7d1f68-304f-404e-ad42-e44d8fcd7641",
    "_execution_state": "idle",
    "_uuid": "62fa87cd6459836228f94b1b5797f2c4d6e17ea7"
   },
   "outputs": [],
   "source": [
    "# Draw a heatmap with the numeric values in each cell\n",
    "f, ax = plt.subplots(figsize=(50, 30))\n",
    "sns.heatmap(train_day, annot=False, ax=ax, fmt=\"d\", linewidths=2)\n",
    "plt.title('Web Traffic Months cross days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6af8cc60-2452-4a32-8861-edcc3118e72e",
    "_uuid": "31e2a53e0513ace60c416f2bd6bf4239b3799baf"
   },
   "source": [
    "With this graph it is possible to see they are two periods with a bigger activity than the rest. The two periods are 25-29 December and 13-14 November. And we can see one period with little activity 15-17 December. They are maybe few outliers during these two periods. You must to investigate more. (coming soon...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39259a1c-8256-4466-bc52-7f16d3d1b8e9",
    "_uuid": "cdc6d4206d065cef9f3a5b2ead3b309a45646266"
   },
   "source": [
    "#  <h1><center> III. ML Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2d1de832-db7d-43bb-b1d9-5776ee2a7216",
    "_uuid": "1cd61a808e610195b6b232da475d3166631bbb4a"
   },
   "source": [
    "The first approach introduces is the Machine Learnin Approach. We will use just a AdaBoostRegressor but you can try with other models if you want to find the best model. I tried with a linear model as like Ridge but ADA model is better. I will be interesting to check if GB or XGB can bit ADA. It is possible to do a Neural Network approach too. But this approach will be done if the kagglers want more !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "33193763-6526-48fa-8229-3dacbe5cc093",
    "_execution_state": "idle",
    "_uuid": "144977197355b75bd9f62555ac23903c9cf40db0"
   },
   "outputs": [],
   "source": [
    "times_series_means =  pd.DataFrame(mean_group).reset_index(drop=False)\n",
    "times_series_means['weekday'] = times_series_means['date'].apply(lambda x: x.weekday())\n",
    "times_series_means['Date_str'] = times_series_means['date'].apply(lambda x: str(x))\n",
    "times_series_means[['year','month','day']] = pd.DataFrame(times_series_means['Date_str'].str.split('-',2).tolist(), columns = ['year','month','day'])\n",
    "date_staging = pd.DataFrame(times_series_means['day'].str.split(' ',2).tolist(), columns = ['day','other'])\n",
    "times_series_means['day'] = date_staging['day']*1\n",
    "times_series_means.drop('Date_str',axis = 1, inplace =True)\n",
    "times_series_means.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dd76ca56-f848-44ca-82ab-c4a1f463bfbb",
    "_uuid": "6b198b328a91a5ff88f53548971dae632cfd70c5"
   },
   "source": [
    "The first step for the ML approach is to create the feature that we will predict. In our example we don't predict the number of visits but the difference between two days. The tips to create few features is to take the difference between two days and to do a lag. Here we will take a lag of \"diff\" seven times. If you have a weekly pattern it is an interesting choice. Here we have few data (2 months so 30 values) and it is a contraint. I done some test and the number 7 is a good choice (weekly pattern?).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "aef98779-9afd-4b72-90af-893bcc8678e5",
    "_execution_state": "idle",
    "_uuid": "45d98aa8df019129e81f7805615e55d278e0f279",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_series_means.reset_index(drop=True,inplace=True)\n",
    "\n",
    "def lag_func(data,lag):\n",
    "    lag = lag\n",
    "    X = lagmat(data[\"diff\"], lag)\n",
    "    lagged = data.copy()\n",
    "    for c in range(1,lag+1):\n",
    "        lagged[\"lag%d\" % c] = X[:, c-1]\n",
    "    return lagged\n",
    "\n",
    "def diff_creation(data):\n",
    "    data[\"diff\"] = np.nan\n",
    "    data.ix[1:, \"diff\"] = (data.iloc[1:, 1].as_matrix() - data.iloc[:len(data)-1, 1].as_matrix())\n",
    "    return data\n",
    "\n",
    "df_count = diff_creation(times_series_means)\n",
    "\n",
    "# Creation of 7 features with \"diff\"\n",
    "lag = 7\n",
    "lagged = lag_func(df_count,lag)\n",
    "last_date = lagged['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "34b1ad68-e648-4b41-84af-7c9d7ea60f43",
    "_execution_state": "idle",
    "_uuid": "cfe4c4517c7cab8865fc073287159508c019e530"
   },
   "outputs": [],
   "source": [
    "lagged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "4eb02453-9d2c-4268-9cb3-20c5d0572b0f",
    "_execution_state": "idle",
    "_uuid": "a5a989090120ad4b63d11782446db7d8e080d85a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Test split\n",
    "def train_test(data_lag):\n",
    "    xc = [\"lag%d\" % i for i in range(1,lag+1)] + ['weekday'] + ['day']\n",
    "    split = 0.70\n",
    "    xt = data_lag[(lag+1):][xc]\n",
    "    yt = data_lag[(lag+1):][\"diff\"]\n",
    "    isplit = int(len(xt) * split)\n",
    "    x_train, y_train, x_test, y_test = xt[:isplit], yt[:isplit], xt[isplit:], yt[isplit:]\n",
    "    return x_train, y_train, x_test, y_test, xt, yt\n",
    "\n",
    "x_train, y_train, x_test, y_test, xt, yt = train_test(lagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "5cb028be-8dd9-40f2-bf0f-3e5f32ccff92",
    "_execution_state": "idle",
    "_uuid": "9d40a516354837b466526f137a0273707ed66d15"
   },
   "outputs": [],
   "source": [
    "# Linear Model\n",
    "from sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor, BaggingRegressor, AdaBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "def modelisation(x_tr, y_tr, x_ts, y_ts, xt, yt, model0, model1):\n",
    "    # Modelisation with all product\n",
    "    model0.fit(x_tr, y_tr)\n",
    "\n",
    "    prediction = model0.predict(x_ts)\n",
    "    r2 = r2_score(y_ts.as_matrix(), model0.predict(x_ts))\n",
    "    mae = mean_absolute_error(y_ts.as_matrix(), model0.predict(x_ts))\n",
    "    print (\"-----------------------------------------------\")\n",
    "    print (\"mae with 70% of the data to train:\", mae)\n",
    "    print (\"-----------------------------------------------\")\n",
    "\n",
    "    # Model with all data\n",
    "    model1.fit(xt, yt) \n",
    "    \n",
    "    return model1, prediction, model0\n",
    "\n",
    "model0 =  AdaBoostRegressor(n_estimators = 5000, random_state = 42, learning_rate=0.01)\n",
    "model1 =  AdaBoostRegressor(n_estimators = 5000, random_state = 42, learning_rate=0.01)\n",
    "\n",
    "clr, prediction, clr0  = modelisation(x_train, y_train, x_test, y_test, xt, yt, model0, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "063577b4-44b4-43ff-920c-009eb5f7bdff",
    "_execution_state": "idle",
    "_uuid": "12ee028196b6aec13ffe162035a7fa11275390c5"
   },
   "outputs": [],
   "source": [
    "# Performance 1\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(50, 12))\n",
    "line_up, = plt.plot(prediction,label='Prediction')\n",
    "line_down, = plt.plot(np.array(y_test),label='Reality')\n",
    "plt.ylabel('Series')\n",
    "plt.legend(handles=[line_up, line_down])\n",
    "plt.title('Performance of predictions - Benchmark Predictions vs Reality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "0e67e975-d6b4-4fe7-afcf-401e4ddcdc6b",
    "_execution_state": "idle",
    "_uuid": "19fe6f013a6199bc370ffd49430c368b66f23e78",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "def pred_df(data,number_of_days):\n",
    "    data_pred = pd.DataFrame(pd.Series(data[\"date\"][data.shape[0]-1] + timedelta(days=1)),columns = [\"date\"])\n",
    "    for i in range(number_of_days):\n",
    "        inter = pd.DataFrame(pd.Series(data[\"date\"][data.shape[0]-1] + timedelta(days=i+2)),columns = [\"date\"])\n",
    "        data_pred = pd.concat([data_pred,inter]).reset_index(drop=True)\n",
    "    return data_pred\n",
    "\n",
    "data_to_pred = pred_df(df_count,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "7b2e0ec3-cb52-463c-95cc-bf0b0eaffd50",
    "_execution_state": "idle",
    "_uuid": "8ffcd2993fc27bb66e3de893d099b30a6168ab6a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialisation(data_lag, data_pred, model, xtrain, ytrain, number_of_days):\n",
    "    # Initialisation\n",
    "    model.fit(xtrain, ytrain)\n",
    "    \n",
    "    for i in range(number_of_days-1):\n",
    "        lag1 = data_lag.tail(1)[\"diff\"].values[0]\n",
    "        lag2 = data_lag.tail(1)[\"lag1\"].values[0]\n",
    "        lag3 = data_lag.tail(1)[\"lag2\"].values[0]\n",
    "        lag4 = data_lag.tail(1)[\"lag3\"].values[0]\n",
    "        lag5 = data_lag.tail(1)[\"lag4\"].values[0]\n",
    "        lag6 = data_lag.tail(1)[\"lag5\"].values[0]\n",
    "        lag7 = data_lag.tail(1)[\"lag6\"].values[0]\n",
    "        lag8 = data_lag.tail(1)[\"lag7\"].values[0]\n",
    "        \n",
    "        data_pred['weekday'] = data_pred['date'].apply(lambda x:x.weekday())\n",
    "        weekday = data_pred['weekday'][0]\n",
    "        \n",
    "        row = pd.Series([lag1,lag2,lag3,lag4,lag5,lag6,lag7,lag8,weekday]\n",
    "                        ,['lag1', 'lag2', 'lag3','lag4','lag5','lag6','lag7','lag8','weekday'])\n",
    "        to_predict = pd.DataFrame(columns = ['lag1', 'lag2', 'lag3','lag4','lag5','lag6','lag7','lag8','weekday'])\n",
    "        prediction = pd.DataFrame(columns = ['diff'])\n",
    "        to_predict = to_predict.append([row])\n",
    "        prediction = pd.DataFrame(model.predict(to_predict),columns = ['diff'])\n",
    "\n",
    "        # Loop\n",
    "        if i == 0:\n",
    "            last_predict = data_lag[\"Visits\"][data_lag.shape[0]-1] + prediction.values[0][0]\n",
    "\n",
    "        if i > 0 :\n",
    "            last_predict = data_lag[\"Visits\"][data_lag.shape[0]-1] + prediction.values[0][0]\n",
    "        \n",
    "        data_lag = pd.concat([data_lag,prediction.join(data_pred[\"date\"]).join(to_predict)]).reset_index(drop=True)\n",
    "        data_lag[\"Visits\"][data_lag.shape[0]-1] = last_predict\n",
    "        \n",
    "        # test\n",
    "        data_pred = data_pred[data_pred[\"date\"]>data_pred[\"date\"][0]].reset_index(drop=True)\n",
    "        \n",
    "    return data_lag\n",
    "\n",
    "model_fin = AdaBoostRegressor(n_estimators = 5000, random_state = 42, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "9eadbee9-2f3e-434b-9cbd-2755d9f63830",
    "_execution_state": "idle",
    "_uuid": "aaf8c8188c6832192937fd7cec0c0c5913dad80d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lagged = initialisation(lagged, data_to_pred, model_fin, xt, yt, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "bcae573e-0c40-4d0d-980e-6a28e53114d8",
    "_execution_state": "idle",
    "_uuid": "124382af7a4b29655e6dc291649ae24faf62bb86",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lagged[lagged['diff']<0]\n",
    "lagged.ix[(lagged.Visits < 0), 'Visits'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "9651d85f-c19b-41a6-a5ce-b5caaa468041",
    "_execution_state": "idle",
    "_uuid": "fa19b3959558271ed0590767afea06cb9e3c5686"
   },
   "outputs": [],
   "source": [
    "df_lagged = lagged[['Visits','date']]\n",
    "df_train = df_lagged[df_lagged['date'] <= last_date]\n",
    "df_pred = df_lagged[df_lagged['date'] >= last_date]\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(30, 5))\n",
    "plt.plot(df_train.date,df_train.Visits)\n",
    "plt.plot(df_pred.date,df_pred.Visits,color='b')\n",
    "plt.title('Training time series in red, Prediction on 30 days in blue -- ML Approach')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1abc551b-5f6c-4626-a4b0-2bd89d6528a4",
    "_uuid": "585458e1d8136ef6e1a8d5c04be13473aee3ea9f"
   },
   "source": [
    "Finshed for the first approach ! The ML method requires a lot of work ! You need to create the features, the data to collect the prediction, optimisation etc... This method done a good results when there are a weekly pattern identified or a monthly pattern but we need more data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b5c5b551-5089-4f49-aac4-3596629e1324",
    "_uuid": "101bd1c76c98cecc6b4e5542021bad2777286898"
   },
   "source": [
    "#  <h1><center> IV. Basic Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dec9bc45-6c65-44a0-8117-da98808ae1aa",
    "_uuid": "d94cbbc753e9fb2840784ecb70613a1f66c64de1"
   },
   "source": [
    "For this model We will use a simple model with the average of the activity by weekdays. In general rules the simplest things give good results !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "32dba46e-01ea-457c-ab05-5f91898e03d0",
    "_execution_state": "idle",
    "_uuid": "f1967adc8656c03da202b8bf1190a8f59eec5581",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lagged_basic = lagged[['date','Visits','weekday']]\n",
    "lagged_basic_tr   = lagged_basic[lagged_basic['date'] < last_date]\n",
    "lagged_basic_pred = lagged_basic[lagged_basic['date'] >= last_date]\n",
    "lagged_basic_pred.drop('Visits',inplace=True,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "d4696f3f-9bab-4389-83b0-d81302d81455",
    "_execution_state": "idle",
    "_uuid": "01c776760d052a9c9a2b9d77724e619eb848934b"
   },
   "outputs": [],
   "source": [
    "prediction_by_days = pd.DataFrame(lagged_basic.groupby(['weekday'])['Visits'].mean())\n",
    "prediction_by_days.reset_index(drop=False,inplace=True)\n",
    "prediction_by_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "3b3356ac-6db0-4a4c-8c84-3f1729d817c9",
    "_execution_state": "idle",
    "_uuid": "ff0de3966feb9f5e0df949f7291c531d8ac14e4c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basic_pred = pd.merge(lagged_basic_pred,prediction_by_days,on='weekday')\n",
    "basic_approach = pd.concat([lagged_basic_tr,basic_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "58f3ce03-df92-43b2-94b2-b7f6e0cee386",
    "_execution_state": "idle",
    "_uuid": "5a5a24033764ccb801bdbff09914642c01e4957e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_basic = np.array(basic_approach[basic_approach['date'] > last_date].sort_values(by='date').Visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "75ff52cb-7f34-480e-8e13-ef7d93e7989f",
    "_execution_state": "idle",
    "_uuid": "da1a5fb96e334e59f348d2e1a3f3c6ff5de3c7c9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 5))\n",
    "plt.plot(plot_basic)\n",
    "plt.title('Display the predictions with the Basic model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "0ee605ca-655b-4d5f-96ba-e1338e90b05f",
    "_execution_state": "idle",
    "_uuid": "462e73413b71e1c702bfd9f4edf61b48ed897b1d"
   },
   "outputs": [],
   "source": [
    "df_lagged = basic_approach[['Visits','date']].sort_values(by='date')\n",
    "df_train = df_lagged[df_lagged['date'] <= last_date]\n",
    "df_pred = df_lagged[df_lagged['date'] >= last_date]\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(30, 5))\n",
    "plt.plot(df_train.date,df_train.Visits)\n",
    "plt.plot(df_pred.date,df_pred.Visits,color='b')\n",
    "plt.title('Training time series in red, Prediction on 30 days in blue -- ML Approach')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "68fe44a2-6737-4a84-bdec-797f4794e21e",
    "_uuid": "326ccff7a28fb783e340b2243cfe379514ae891c"
   },
   "source": [
    "No optimisation ! No choice between linear, Bagging, boosting or others ! Just with an average by week days and we have a result ! Fast and easily !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8dc40827-da3d-4808-a442-511f42fb64d5",
    "_uuid": "95617379ee862f8a8799be18619c8c2b5cf6eb12"
   },
   "source": [
    "#  <h1><center>V. ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "294b1abe-e51d-4f13-bdc8-5947003795ab",
    "_uuid": "3e491bf7ab5d1824df13a100bc82da2a51447823"
   },
   "source": [
    "This part is inspired by: https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\n",
    "Very goodjob with the ARIMA models ! It is more simple when we have directly a stationary Time series. It is not our case...\n",
    "\n",
    "We will use the Dickey-Fuller Test. More informations here: https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "e05ae19a-9cac-4791-9e31-3532a72e605c",
    "_execution_state": "idle",
    "_uuid": "e498e0a691203c0b51e16412a33719ae3b39e0c7"
   },
   "outputs": [],
   "source": [
    "# Show Rolling mean, Rolling Std and Test for the stationnarity\n",
    "df_date_index = times_series_means[['date','Visits']].set_index('date')\n",
    "\n",
    "def test_stationarity(timeseries):\n",
    "    plt.figure(figsize=(50, 8))\n",
    "    #Determing rolling statistics\n",
    "    rolmean = pd.rolling_mean(timeseries, window=7)\n",
    "    rolstd = pd.rolling_std(timeseries, window=7)\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = sm.tsa.adfuller(timeseries['Visits'], autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)\n",
    "    \n",
    "test_stationarity(df_date_index)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ff451655-0c1c-43c9-97e8-01fcb02746ba",
    "_uuid": "f7522f7ea1e0ffc4d68939a2b45c549448621680"
   },
   "source": [
    "Our Time Series is stationary ! it is a good news ! We can to apply the ARIMA Model without transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ecb90532-9f4a-4e86-aa21-8c1ab790eb92",
    "_uuid": "38e7989848f06722bcae75aaf4402e3dc5ccaa90"
   },
   "source": [
    "Good job ! We have a Time Series Stationary ! We can apply our ARIMA Model !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "0b1e2083-b7bb-48b0-87ea-8cca044b7fef",
    "_execution_state": "idle",
    "_uuid": "c203f4e9efe8267b85e1d868fbca077bd542ee6a"
   },
   "outputs": [],
   "source": [
    "# Naive decomposition of our Time Series as explained above\n",
    "decomposition = sm.tsa.seasonal_decompose(df_date_index, model='multiplicative',freq = 7)\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "rcParams['figure.figsize'] = 30, 20\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.title('Obesered = Trend + Seasonality + Residuals')\n",
    "plt.plot(df_date_index, label='Observed')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1549b177-9f22-44c6-a56f-f15e086e6e92",
    "_execution_state": "idle",
    "_uuid": "037f1e97a06a7d8fc49a46f7a2d404dd412d407c"
   },
   "source": [
    "We expose the naive decomposition of our time series (More sophisticated methods should be preferred). They are several ways to decompose a time series but in our example we take a simple decomposition on three parts.\n",
    "The additive model is Y[t] = T[t] + S[t] + e[t]\n",
    "The multiplicative model is Y[t] = T[t] x S[t] x e[t]\n",
    "with:\n",
    "\n",
    " 1. T[t]: Trend \n",
    " 2. S[t]: Seasonality \n",
    " 3. e[t]: Residual\n",
    "\n",
    "An additive model is linear where changes over time are consistently made by the same amount. A linear trend is a straight line. A linear seasonality has the same frequency (width of cycles) and amplitude (height of cycles).\n",
    "A multiplicative model is nonlinear, such as quadratic or exponential. Changes increase or decrease over time. A nonlinear trend is a curved line.A non-linear seasonality has an increasing or decreasing frequency and/or amplitude over time.\n",
    "In ou example we can see it is not a linear model. So it is the reason why we use a multiplicative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a17a83fc-a8d0-400a-ab01-0d391e3c3b17",
    "_execution_state": "idle",
    "_uuid": "724736b90f1bfcbb0e8f79e05ebe689fbf890989",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "#model = ARIMA(df_date_index, order=(7, 1, 0))  \n",
    "#results_AR = model.fit(disp=-1)  \n",
    "#plt.plot(df_date_index, color = 'blue')\n",
    "#plt.plot(results_AR.fittedvalues, color='red')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "888d1ecd-74fb-45a9-83f6-06dd80558dbc",
    "_execution_state": "idle",
    "_uuid": "3c01094c465a980ca0333a1709f8e3c58af64ccd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#forecast = results_AR.forecast(steps = 30)[0]\n",
    "#plt.figure(figsize=(30, 5))\n",
    "#plt.plot(pd.DataFrame(np.exp(forecast)))\n",
    "#plt.title('Display the predictions with the ARIMA model')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7cc3590f-7e5a-4c8e-9e47-a442ed818431",
    "_execution_state": "idle",
    "_uuid": "00eee92cfb2cd72ea1a797b1684aee1278021056",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DataFrame to collect the predictions\n",
    "#df_prediction_arima = df_date_index.copy()\n",
    "\n",
    "#list_date = []\n",
    "#for i in range(31):\n",
    "#    if i >0:\n",
    "#        list_date.append(last_date  + pd.to_timedelta(i, unit='D'))\n",
    "    \n",
    "#predictions_arima = pd.DataFrame(list_date,columns = ['Date'])\n",
    "#predictions_arima['Visits'] = 0\n",
    "#predictions_arima.set_index('Date',inplace=True)\n",
    "#predictions_arima['Visits'] = np.exp(forecast)\n",
    "\n",
    "#df_prediction_arima = df_prediction_arima.append(predictions_arima)\n",
    "#df_prediction_arima.reset_index(drop=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d5978868-0f48-438c-a023-8b4491778afa",
    "_execution_state": "idle",
    "_uuid": "50ce9cf6b258a09de8a27d997f86c4646df554fa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_arima = df_prediction_arima[['Visits','index']]\n",
    "#df_train = df_arima[df_arima['index'] <= last_date]\n",
    "#df_pred = df_arima[df_prediction_arima['index'] >= last_date]\n",
    "#plt.style.use('ggplot')\n",
    "#plt.figure(figsize=(30, 5))\n",
    "#plt.plot(df_train.index,df_train.Visits)\n",
    "#plt.plot(df_pred.index,df_pred.Visits,color='b')\n",
    "#plt.title('Training time series in red, Prediction on 30 days in blue -- ARIMA Model')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d942d9b2-cc2e-4cba-8e90-fdb7f6e6b9f5",
    "_execution_state": "idle",
    "_uuid": "816db33bfd08a412dc64ce685ba0423931f68b96"
   },
   "source": [
    "IN PROGRESS...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3c258329-00bc-4495-9075-2029db491b76",
    "_uuid": "9219d1f194fbd3142a54591995dea7daed5956f6"
   },
   "source": [
    "#  <h1><center>VI. Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5520e5bc-dee5-485a-b9b5-c12525cd71b0",
    "_uuid": "8efb091700dede1713d40a09d4114284a11948d0",
    "collapsed": true
   },
   "source": [
    "Prophet is a forecasting tool availaible in python and R. This tool was created by Facebook. More information on the library here: https://research.fb.com/prophet-forecasting-at-scale/\n",
    "\n",
    "Compared to the two methods this one will be faster. We can forecast a time series with few lines. In our case we will do a forecast and a display the trend of activity on the period and for a week.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_cell_guid": "a7093f58-f499-4012-b172-046ec83ad163",
    "_execution_state": "idle",
    "_uuid": "b155c97db5268ea7907cbea3e5f2d0c63f9194d8"
   },
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "sns.set(font_scale=1) \n",
    "df_date_index = times_series_means[['date','Visits']]\n",
    "df_date_index = df_date_index.set_index('date')\n",
    "df_prophet = df_date_index.copy()\n",
    "df_prophet.reset_index(drop=False,inplace=True)\n",
    "df_prophet.columns = ['ds','y']\n",
    "\n",
    "m = Prophet()\n",
    "m.fit(df_prophet)\n",
    "future = m.make_future_dataframe(periods=30,freq='D')\n",
    "forecast = m.predict(future)\n",
    "fig = m.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "febe2599-0562-4a25-aa57-5dbae1028d66",
    "_execution_state": "idle",
    "_uuid": "1f3f1b568c852f0b8b5c6cd5568ce7e56430a07a"
   },
   "outputs": [],
   "source": [
    "m.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4edeeb1e-806d-451c-95cb-d57724d4afc7",
    "_uuid": "2b0cde46458878b19e908712aa8d09b823fc6339"
   },
   "source": [
    "#  <h1><center>VI. Keras Starter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a277b232-12d1-42fc-ac2f-c9f9c0cb9f88",
    "_uuid": "bd795ce9142e371ab271ad1fe075f04463298b73"
   },
   "source": [
    "In this part we will use Keras without optimisation to forecast. It is just a very simple code to begin with Keras and a Time Series. For our example we will try just with one layer and 8 Neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "b4dc5983-5cd6-47b7-83e2-1a56e13d501f",
    "_execution_state": "idle",
    "_uuid": "d65e9615d086128d6ef4a6d80d777446233e4c8a"
   },
   "outputs": [],
   "source": [
    "df_dl = times_series_means[['date','Visits']]\n",
    "\n",
    "train_size = int(len(df_dl) * 0.80)\n",
    "test_size = len(df_dl) - train_size\n",
    "train, test = df_dl.iloc[0:train_size,:], df_dl.iloc[train_size:len(df_dl),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "12958f39-3fa2-4fcf-808d-1766af6a79d0",
    "_execution_state": "idle",
    "_uuid": "f696c5cc2abc31e2cefd2141abff6589d1a28ae1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "look_back = 1\n",
    "\n",
    "def create_dataset(dataset, look_back):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset.iloc[i:(i+look_back), 1].values[0]\n",
    "        b = dataset.iloc[i+look_back, 1]\n",
    "        dataX.append(a)\n",
    "        dataY.append(b)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "trainX, trainY = create_dataset(train, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "75c23e77-9086-458a-b9e6-52be961ba490",
    "_execution_state": "idle",
    "_uuid": "f0ccbc272f7e1ef22c1b17fea81bdf32b1bbec36"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=look_back, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=150, batch_size=2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "3f41530f-d90d-41c2-9a29-d329db4823cc",
    "_execution_state": "idle",
    "_uuid": "0294900a2ca8e9b0c601e76dcb4ffaa3e0cfd443"
   },
   "outputs": [],
   "source": [
    "trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
    "print('Train Score: %.2f MSE (%.2f MAE)' % (trainScore, trainScore))\n",
    "testScore = model.evaluate(testX, testY, verbose=0)\n",
    "print('Test Score: %.2f MSE (%.2f MAE)' % (testScore, testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "6c7b4795-7106-4400-90ce-d18b9590b370",
    "_execution_state": "idle",
    "_uuid": "2a99a913dfa821a3fed3965758ebfa986e9f1dd4"
   },
   "outputs": [],
   "source": [
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    " \n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(df_dl)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    " \n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(df_dl)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(df_dl)-1, :] = testPredict\n",
    " \n",
    "# plot baseline and predictions\n",
    "plt.plot(np.array(df_dl.Visits))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.title('Predicition with Keras')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e4f6b92a-6136-4691-9237-2e3d6ae4be0c",
    "_uuid": "33bf2b138a387a00ef6d678b6faad9febc52e796"
   },
   "source": [
    " # <h1><center>VII. Comparison & Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0fddb93d-6832-445c-a1fe-e5fd27dba030",
    "_uuid": "7f0ec58c805554010155f32258c62241104fe168",
    "collapsed": true
   },
   "source": [
    "In Progress..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
